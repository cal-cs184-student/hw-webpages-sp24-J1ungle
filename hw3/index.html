<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Yuxiang Jiang</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="https://cal-cs184-student.github.io/hw-webpages-sp24-J1ungle/hw3/index.html">here</a></h2>

<br><br>


<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/Part5/CBspheres_lambertian_screenshot_3-13_10-38-23.png" width="480px" />
      </tr>
  </table>
</div>
<div>

<h2 align="middle">Overview</h2>
<p>
  In this project, I've developed a path tracing rendering program aimed at simulating the transport of light to create a convincing and lifelike rendering of 3D scenes. Leveraging various acceleration data structures and mathematical techniques, I've optimized the rendering process for efficiency.<br>

  Despite the demanding workload, I find this project incredibly rewarding. Witnessing the rendered images emerge after long time of rendering never fails to astonish me. The level of detail and realism achieved is truly remarkable, making all the effort invested in the project worthwhile.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>
  <div align="center">
    <table style="width=100%">
        <tr>
            <td align="middle">
            <img src="images/Part 1/camera space.png" width="500px" />
            <figcaption align="middle">Ray tracing model</figcaption>
        </tr>
    </table>
  </div>
  Using the pinhole camera model, to get an image of 3D spaces, we first put the image plane in front of the eye(camera). The value of a
specific pixel is determined by the integral of radiance going through it. But instead of calculating all the light bouncing and reflecting in the scene, we only need to generate a ray from the eye through that pixel, and calculte the integral of radiance in the opposite direction. This is how ray generation works. In practise, we randomly generate several rays through a specific pixel, and use Monte Carlo Integration to estimate the randiance falls on that pixel.<br>
In order to correctly render shadows and calculate the reflection of light, we need to check the intersection between rays and primitives in the world. This also helps to solve the visibility problem. Every ray will store its closest intersection to the eye. And the value will be updated only if a closer intersection is found when checking all the primitives.
</p>
<br>

<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p>
  <div align="center">
    <table style="width=100%">
        <tr>
            <td align="middle">
            <img src="images/Part 1/intersection.png" width="330px" />

        </tr>
    </table>
  </div>
  Every triangle is in a plane. To check intersection to a triangle, we first check intersection to this plane. This can be calculated by solving the equations of the plane and the ray. After we get the coordinate of the intersection point, we need to do some extra work to check whether this point is inside the triangle. An easy way is to calculate the area(cross product) of three triangles formed by the intersection point and other three vertices. If the sum of these three triangles areas equals to the area of the original triangle, this point is contained. Otherwise it is outside the triangle.
</p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:90%">
    <tr align="center">
      <td>
        <img src="images/Part 1/part1banana.png" align="middle" width="400px"/>
        <figcaption>Banana.dae</figcaption>
      </td>
      <td>
        <img src="images/Part 1/part1beetle.png" align="middle" width="400px"/>
        <figcaption>Beetle.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part 1/part1cow.png" align="middle" width="400px"/>
        <figcaption>Cow.dae</figcaption>
      </td>
      <td>
        <img src="images/Part 1/part1teapot.png" align="middle" width="400px"/>
        <figcaption>Teapot.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
  <div align="center">
    <table style="width=100%">
        <tr>
            <td align="middle">
            <img src="images/Part 2/BVH.png" width="350px" />

        </tr>
    </table>
  </div>
    In the BVH construction algorithm, each node contains a list of objects. The algorithm devides these objects into disjoint subsets, assigns these subsets into separate child nodes, and continues to devide until there are just a few objects in a set. In my implementation, I first calculate the bounding box for all objects stored in current node. And choose the axis where bounding box has the largest extent to split. The midpoint of bounding box along that axis is chosen as the splitting point. This is a good heuristic which is also cheap to compute. It chooses the axis along which those objects scatter most and makes the split much more effective.
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:90%">
    <tr align="center">
      <td>
        <img src="images/Part 2/part2blob.png" align="middle" width="400px"/>
        <figcaption>Blob.dae</figcaption>
      </td>
      <td>
        <img src="images/Part 2/part2cblucy.png" align="middle" width="400px"/>
        <figcaption>CBlucy.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part 2/part2dragon.png" align="middle" width="400px"/>
        <figcaption>Dragon.dae</figcaption>
      </td>
      <td>
        <img src="images/Part 2/part2walle.png" align="middle" width="400px"/>
        <figcaption>Wall-E.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
  I rendered CBlucy.dae and Blob.dae with and without BVH acceleration, and the result render time differs enormously. It took 151 seconds and 238 seconds separately to render CBlucy and Blob without acceleration. However, both files took only 0.04 seconds to render with BVH acceleration. For CBlucy.dae, large part of the space is empty except for the exquisite sculpture in the middle. Thus, with BVH acceleration, we managed to avoid checking intersection with large number of triangles on the sculpture for most rays. And for Blob.dae, BVH acceleration also helps enormously reduce the number of triangles we need to check for intersection. This is the reason why we can get such a huge improvement.
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
  Direct Lighting considers the light reflected into the camera which only bounces once on the objects. We implements two ways of direct lighting sampling: Uniform Hemisphere sampling and Importance Sampling. In Uniform Hemisphere Sampling, we sampled incident light direction uniformly on a hemisphere around the bouncing point. But in Importance Sampling, we sampled lights directly from the light sources to the bounce point. The remain parts are basically the same. For Uniform Sampling, we check whether the sampled ray really reaches a light source. For Important Sampling, we also need to check if the light is blocked by some other objects in the middle. The radiance of the ray is set to zero if it is blocked(or not reach the light source). At last, we compute Monte Carlo integration using the sampled ray we derived, the BRDF of the bouncing surface, pdf(different for the two sampling method) of solid angle, and the intersection angle. Finally we get the radiance our eye will see with only direct lighting. Uniform Hemisphere Sampling is not guaranteed to sample direction toward the light source. Thus many samples will contribute nothing to the Monte Carlo intergration and causes an inefficiency. In contrast, Importance Sampling the light can make a greater use of samples, thus result in a less noisy picture with the same number of samples.
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/Part3/ballUniform.png" align="middle" width="400px"/>
        <figcaption>Ball1</figcaption>
      </td>
      <td>
        <img src="images/Part3/ballImportance.png" align="middle" width="400px"/>
        <figcaption>Ball2</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/Part3/bunnyUniform.png" align="middle" width="400px"/>
        <figcaption>Bunny1</figcaption>
      </td>
      <td>
        <img src="images/Part3/bunnyImportance.png" align="middle" width="400px"/>
        <figcaption>Bunny2</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part3/bunny1.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray </figcaption>
      </td>
      <td>
        <img src="images/Part3/bunny4.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays </figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part3/bunny16.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays </figcaption>
      </td>
      <td>
        <img src="images/Part3/bunny64.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays </figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
    The noise level in soft shadows decrease when we increase the number of light samples.
</p>
<br>

<h3>

  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
  Sampling light with importance sampling reduces noise compared to uniformly sampling the hemisphere with the same parameters. The noise present in uniform hemisphere sampling is larger because only a small fraction of the rays emitted from the hit point directly intersect a light source, and most of the sampled radiance is zero. Conversely, importance sampling only samples those directions toward light sources, resulting in smoother renderings because a larger part of the sample rays do hit the light source.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
  Indirect lighting calculates the radiance of light as it reflects multiple times within the scene before reaching our eyes. This technique enhances the rendering with richer and more realistic visual effects compared to using direct lighting alone. The indirect lighting function is implemented in the following steps:
  <ul>
    <li>
      Given a traced ray and the intersection point, compute the direct lighting from the light sources to this intersection point. If we have reached the max depth of the ray, return immediately.
      </li>
    <li>
      Flip a coin, with a probability of 0.4, end the traced ray at current depth(Russian roulette). If ray tracing is to be continued, randomly sample a ray fleeing current intersection point.
      </li>
    <li>
       If the new ray do intersect with other object in the scene, we recursively call this indirect lighting function on the new ray and get the incoming randiance of this ray. Then we compute the reflected light by mutiply this indirect light's radiance with bsdf function as well as the cosine with normal vector of the surface. We additionally divide it by the pdf and Russian roulette continuation probability.
      </li>
    <li>
      Finally, we combine the reflected radiance of indirect lighting at the intersection point with the initially obtained direct lighting value.
      </li>
    </ul>
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part4/ballDirect.png" align="middle" width="400px"/>
        <figcaption>Direct illumination</figcaption>
      </td>
      <td>
        <img src="images/Part4/ball4global.png" align="middle" width="400px"/>
        <figcaption>Global illumination with ray depth 4</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part4/ballDirect.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination </figcaption>
      </td>
      <td>
        <img src="images/Part4/ballOnlyindirect.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (ray depth = 2)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  Above are the rendering of some spheres with only direct illumination and only indirect lighting of depth 2.Comparing the shadows in these two images, we observe that areas underneath the spheres, which are shadowed in the direct illumination scenario, are illuminated by the indirect lighting. Despite being inaccessible directly from the light source, these areas are not completely dark due to light reflecting between surfaces and eventually reaching them.
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 5 (the -m flag), and isAccumBounces=false. Explain in your writeup what you see for the 2nd and 3rd bounce of light, and how it contributes to the quality of the rendered image compared to rasterization. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part4/bunny0Noacc.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part4/bunny1Noacc.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part4/bunny2Noacc.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part4/bunny3Noacc.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part4/bunny4noacc.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 4 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part4/bunny5noacc.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 5 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  When we talked about Blinn-Phong Reflection model in the rasterization pipeline, we just add some constant color to fill in the black shadows to simulate the ambient light. In this way, there's no differences in the lighting of shadows in different places, which is contrary to reality. From the 2nd and 3rd picture above we can clearly see a difference in the abient light in different places. Using this global light transport simulation, we can get a much more realistic and convincing rendering result.
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, 4 and 5. Use 1024 samples per pixel.
</h3>
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part4/bunny0acc.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part4/bunny1acc.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part4/bunny2acc.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part4/bunny3acc.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part4/bunny4acc.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 4 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part4/bunny5acc.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 5 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
  The biggest improvement of rendering occurs between max_ray_depth of 1 and 2. When max_ray_depth is set to one, it is just direct illumination. The shadows look so sharp, and the ceiling is completely dark. After considering two bounces of light in the scene, the shadows and ceiling are lit up by the ambient lights. Due to decrease of radiance after reflection, further increasing of max_ray_depth has little perceptible effect on the scene.
  </p>
<h3>
  For CBbunny.dae, output the Russian Roulette rendering with max_ray_depth set to 0, 1, 2, 3, 4 and 100. Use 1024 samples per pixel.
  </h3>
  <div align="middle">
    <table style="width:100%">
      <tr align="center">
        <td>
          <img src="images/Part4/bunnyRR0.png" align="middle" width="400px"/>
          <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
        </td>
        <td>
          <img src="images/Part4/bunnyRR1.png" align="middle" width="400px"/>
          <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
        </td>
      </tr>
      <tr align="center">
        <td>
          <img src="images/Part4/bunnyRR2.png" align="middle" width="400px"/>
          <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
        </td>
        <td>
          <img src="images/Part4/bunnyRR3.png" align="middle" width="400px"/>
          <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
        </td>
      </tr>
      <tr align="center">
        <td>
          <img src="images/Part4/bunnyRR4.png" align="middle" width="400px"/>
          <figcaption>max_ray_depth = 4 (CBbunny.dae)</figcaption>
        </td>
        <td>
          <img src="images/Part4/bunnyRR100.png" align="middle" width="400px"/>
          <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
        </td>
      </tr>
    </table>
  </div>
  <p>
    In order to make the rendering looks even more realistic, and to avoid infinite recursion of global illumination,   Russian Roulette rendering introduces a probability of ending the recursion at each step in the global illumination function. This saves rendering time, and has little side effect on the rendering result.
  </p>
<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part4/1.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part4/2.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part4/4.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (CBsphere.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part4/8.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (CBsphere.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part4/16.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (CBsphere.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part4/64.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (CBsphere.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part4/1024.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (CBsphere.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    When sample-per-pixel rate is low, the result appears to be noisy. We can clearly see noise even with 64 samples per pixel. So we need to sample much more samples per pixel to get noise under control.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
  To effectively reduce noise in the image, we must increase the number of samples per pixel. Yet, certain areas of the scene converge more rapidly than others. Adaptive sampling capitalizes on this phenomenon by halting the sampling process for a pixel once we possess sufficient confidence to confirm its convergence.<br>
  In my implementation, I check the convergence every <code>samplesPerBatch</code> samples to avoid high computation cost. Given $n$ samples with mean $\mu$ and standard deviation $\sigma$, we compute the value $I$
  $$I = 1.96 \cdot \frac{\sigma}{\sqrt{n}}$$
  If $I \le maxTolerance \cdot \mu$, where $maxTolerance$ is set to $0.05$, we assume that the pixel has converged and stop tracing more rays for this pixel.
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/Part5/CBbunny_screenshot_3-13_11-1-16.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part5/CBbunny_screenshot_3-13_11-1-16_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/Part5/CBspheres_lambertian_screenshot_3-13_10-38-23.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBsphere.dae)</figcaption>
      </td>
      <td>
        <img src="images/Part5/CBspheres_lambertian_screenshot_3-13_10-38-23_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBsphere.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
